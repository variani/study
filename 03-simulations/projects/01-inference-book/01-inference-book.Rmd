---
title: "Exercises  from Little Inference Book"
author: "Andrey Ziyatdinov"
date: "`r Sys.Date()`"
output:
  html_document:
    theme: united
    toc: true
    keep_md: true
---

```{r options, echo = F}
opts_chunk$set(fig.path = "figures/", comment = NA, results = 'markup', tidy = F, message = F, warning = F, echo = T, cache = TRUE)
```

# Include

```{r}
library(pander)
library(ggplot2)

library(tidyverse)
```

## Settings


```{r settings, cache = FALSE}
theme_set(theme_light())

panderOptions('table.style', 'rmarkdown')

panderOptions('table.split.table', Inf)
panderOptions('knitr.auto.asis', FALSE)
```

# Simulations

## The standard error of the mean

What is the variability of the mean of a sample, if we get only a single realization?
(Spoiler: $\sigma^2 / n$.)
[Simulation](https://leanpub.com/LittleInferenceBook/read#leanpub-auto-simulation-example-1-standard-normals)
with repeated sample avergaging can answer the question.

Before going to the code, a small recap:

* The sample variance, $S^2$, estimates the population variance, $\sigma^2$.
* The distribution of the sample variance is centered around $\sigma^2$.
* The variance of the sample mean is $\sigma^2 / n$.

```{r ex1}
nsim <- 1000
n <- 10

set.seed(1)

sim_means <- 
  rep(n, nsim) %>%
  sapply(. %>% rnorm %>% mean)
  
sim_means_sd <- sim_means %>% sd
```

```{r ex1_tab, echo = F, results = "asis", dependson = "ex1"}
tab <- data.frame(method = c("theory: 1 / sqrt(n)", "simulation: sim_means_sd"), sd = c( 1/sqrt(n), sim_means_sd))
pandoc.table(tab, style = 'rmarkdown')
```

```{r ex1_hist, dependson = "ex1"}
ggplot(data.frame(mean = sim_means), aes(mean)) + geom_histogram() + 
  geom_vline(xintercept = c(-sim_means_sd, sim_means_sd)) 
``` 
 
## Confidence interval for the mean

Some references:

* http://www.cyclismo.org/tutorial/R/confidence.html
* http://stackoverflow.com/questions/15180008/how-to-calculate-the-95-confidence-interval-for-the-slope-in-a-linear-regressio


We know that the sample mean is a radom variable.
[The Central Limit Theorem (CLT)](https://leanpub.com/LittleInferenceBook/read#leanpub-auto-the-central-limit-theorem) gives us an idea of its distribution for iid variabels with the increasing sample size.

The CLT says:

$\frac{\bar{X_n} - \mu}{\sigma / \sqrt{n}} = \frac{\mbox{Estimate} - \mbox{Mean of estimate}}{\mbox{Std. Err. of estimate}}$

aproaches to a standard normal distribution for large $n$. Note that replacing the standard error by its estimate doesn't change the theorem.

In short: CLT says that $\bar{X_n}$ is approximately $N(\mu, \sigma^2 / n)$.

### Confidence intervals of the mean

The formula: $\bar{X_n} \pm Z_{1 - \alpha / 2} \frac{\sigma}{\sqrt{n}}$.

For normal distribution:

```{r ci_norm}
n <- 100
x <- rnorm(n)

alpha <- 0.05
(mean(x) + c(-1, 0, 1) * qnorm(alpha/2) * sd(x) / sqrt(n)) %>% round(2)
```

For binomial distribution ($\mu = \hat{p}$ and $\sigma^2 = \hat{p} (1 - \hat{p})$): 

```{r ci_binom}
n <- 100
k <- 56

p <- k / n

alpha <- 0.05
(p + c(-1, 0, 1) * qnorm(alpha/2) * sqrt(p * (1 - p)) / sqrt(n)) %>% round(2)
```

### Simulation of confidence intervals

One can **not** technicaly sat that the CI contains the parameter with probability, e.g. 95% 
(Bayesian credibal intervals address this point).

The CIs described an aggregate behavour: the CIs say the percetage of intervals 
that would include the estimated parameter when repeating the experiment many times.

Simple simulations for binomial data will illustate this notion:

```{r ci_sim}
probs <- seq(0.1, 0.9, by = 0.05)

set.seed(1)

simulate_coverage_trial <- function(n, p, alpha = 0.05)
{
  x <-  rbinom(n, size = 1, prob = p)
  phat <- mean(x)
  
  d <- qnorm(1 - alpha / 2) * sqrt(phat * (1 - phat) / n)
  ll <- phat - d
  ul <- phat + d
  
  return(ll < p & ul > p)
}

simulate_coverage <- function(nsim, n, p, alpha = 0.05)
{
  num_cov_sum <- rep(n, nsim) %>%
    sapply(. %>% simulate_coverage_trial(p, alpha)) %>%
    sum
  
  num_cov_sum / nsim # avr among `nsim` repetitions 
}

sim_cov_n20 <- 
  probs %>%
  sapply(. %>% simulate_coverage(nsim = 1000, n = 20, .))

sim_cov_n200 <- 
  probs %>%
  sapply(. %>% simulate_coverage(nsim = 1000, n = 200, .))
```

```{r plot_bi_cov, echo = F}
tab <- data.frame(p = probs, cov20 = sim_cov_n20, cov200 = sim_cov_n200)

ggplot(tab) + 
  geom_line(aes(p, cov20), linetype = 2) + 
  geom_line(aes(p, cov200)) + 
  geom_hline(yintercept = 1 - alpha, color = "red") +
  ylim(c(0.5, 1)) + 
  labs(x = "True paramter p", y = "Coverage of 95% CI by 1,000 simulations", 
    title = "Solid line: n = 200; Dashed line: n = 20")
```

The bad coverage for the case $n = 20$ demonstrates that 
the CLT applicability for CIs requires a large sample size.




